{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"##### This Python 3 environment comes with many helpful analytics libraries installed\n##### It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n##### For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nimport category_encoders as ce\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn import linear_model\n\n##### Input data files are available in the \"../input/\" directory.\n##### For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n##### Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"#read in the files\nsample_submission = pd.read_csv(\"../input/cat-in-the-dat/sample_submission.csv\")\ntest = pd.read_csv(\"../input/cat-in-the-dat/test.csv\")\ntrain = pd.read_csv(\"../input/cat-in-the-dat/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of training data: {}\" .format(train.shape))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['nom_5','nom_6','nom_7','nom_8','nom_9']].iloc[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bytes.fromhex('b6dd5612').decode(encoding='cp037',errors='ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the features of the dataset consists of mainly **binary**, **nominal**, **ordinal** and **cyclical**(day,month) features <br>\n<br>\n**Binary**: Consists of only two unique values. e.g 1/0, True/False, Yes/No <br>\n**Nominal**: Values that does not have any ordering <br>\n**Ordinal**: Values with specific ordering <br>\n**Cyclical**: Values that are recurrent in nature"},{"metadata":{},"cell_type":"markdown","source":"#### Distribution of Target\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(x='target',y='target', data=train, estimator=lambda x:len(x)/len(train)*100)\nax.set(ylabel =\"Frequency in %\", title=\"distribution of target\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binary Variables <br>\nWe will first investigate the distribution of 0s and 1s in the Binary variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_col = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\n\n\ngrid = gridspec.GridSpec(3, 2)\nplt.figure(figsize=(15,20))\nfor i,col in enumerate(train[bin_col]):\n    ax = plt.subplot(grid[i]) \n    sns.countplot(x=col, data=train) \n    ax.set_ylabel('Count')  \n    ax.set_xlabel('Values') \n    ax.set_title('{} Distribution'.format(col))\n    sizes=[] \n    for p in ax.patches:\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/len(train)*100),\n                ha=\"center\", fontsize=14) \n    ax.set_ylim(0, max(sizes) * 1.15) \n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Preprocessing Binary Variables <br>\nMachine Learning algorithm works on numerical data, hence, we will have to map boolean values from *bin_3* and *bin_4* into 0s and 1s"},{"metadata":{"trusted":true},"cell_type":"code","source":"mapper = {'T': 1, 'F':0, 'Y':1, 'N':0}\n#map char into 0/1\ntrain['bin_3'] = train['bin_3'].map(mapper)\ntest['bin_3'] = test['bin_3'].map(mapper)\ntrain['bin_4'] = train['bin_4'].map(mapper)\ntest['bin_4'] = test['bin_4'].map(mapper)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['bin_3'].value_counts())\nprint(train['bin_4'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{},"cell_type":"markdown","source":"### Nominal Variables <br>\nWe will try to visualize the values and its distribution for the nominal variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"nom_col = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n\nfor col in nom_col:\n    print(\"Number of unique values for {}: {}\".format(col,train[col].nunique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that *nom_5* to *nom_9* has high cardianality (alot of unique values). For visualisation purposes, we will just look at the nominal features with less than 10 unique values[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,20))\ngrid = gridspec.GridSpec(3, 2)\n\nfor i,col in enumerate(train[nom_col[:5]]):\n    ax = plt.subplot(grid[i]) \n    sns.countplot(x=col, data=train) \n    ax.set_ylabel('Count')  \n    ax.set_xlabel('Values') \n    ax.set_title('{} Distribution'.format(col))\n    sizes=[] \n    for p in ax.patches:\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/len(train)*100),\n                ha=\"center\", fontsize=14) \n    ax.set_ylim(0, max(sizes) * 1.15) \n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### One-Hot encoding <br>\nWe will use one-hot encoding in this case for *nom_0* to *nom_4*. One-hot encoding will create K number os columns based on the unique values and assign a binary value indicating the presence of the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.get_dummies(train, prefix=nom_col[0:5], columns=nom_col[0:5])\ntest = pd.get_dummies(test, prefix=nom_col[0:5], columns=nom_col[0:5])\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Nominal Features with high cardianality <br>\nLets look at *nom_5* to *nom_9* values to find out more about the high cardianality features. They are mainly made up of alphanumeic strings that doesnt have any meaning\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # investigate if nom_5 to nom_9 has similar values\n\n# n5 = train[['nom_5']]; n6 = train[['nom_6']]; n7 = train[['nom_7']]; n8 = train[['nom_8']]; n9 = train[['nom_9']]\n# def coll(df):\n#     df.columns = ['nom']\n#     return df\n# long = [n5,n6,n7,n8,n9]\n# nSum = 0\n# for i in long:\n#     i = coll(i)\n#     nSum += i.nunique()\n# print(nSum)\n\n# nom = pd.concat(long,ignore_index=True)\n# print(nom.shape)\n# print(nom.nunique())    #all are distinct with one another","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, i will try to use:\n1. **Frequency Encoding**, which extracts the occurrence of teach strings as feature rpresentation for the nominal features with high cardianlity\n2. **Decoding hexademical**, which decodes the hexa values to 1 and 0. Then split into respective columns as similar to one-hot encoding"},{"metadata":{},"cell_type":"markdown","source":"#### Frequency Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for col in nom_col[5:]:\n#     col_name = col+'_freq_encode'\n#     fe_train = train.groupby(col).size()/len(train)\n#     train[col_name] = train[col].map(fe_train)\n#     fe_test = test.groupby(col).size()/len(test)\n#     test[col_name] = test[col].map(fe_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Hashing"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in nom_col[5:]:\n    col_name = col+'_hash_encode'\n    train[col_name] = train[col].apply(lambda x:hash(str(x)) % 6011)\n    test[col_name] = test[col].apply(lambda x:hash(str(x)) % 6011)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decoding Hexadecimal values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import TargetEncoder\nencoder = TargetEncoder()\nfor col in nom_col[5:]:\n    colname = col+'_target'\n    train[colname] = encoder.fit_transform(train['nom_5'], train['target'])\n    test[colname] = encoder.transform(test['nom_5'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to decode hexa to binary\ndef hex_to_bin(val):\n    base = 16\n    num_bits = 36\n    res = bin(int(val, base))[2:].zfill(num_bits)\n    return str(res)\n\n#split bin values into individual columns\ndef decode_(df):\n    for col in nom_col[5:]:\n        df[col] = df[col].apply(lambda x:hex_to_bin(x))\n        \n    for col in nom_col[5:]:\n        df[col] = df[col].apply(lambda x:list(x))\n        new_col = pd.DataFrame(df[col].values.tolist()).rename(columns = lambda x: col+'_decode_'+str(x))\n        df = pd.concat([df,new_col],axis=1)\n        \n    return df\n\n#run for test and train\ntrain = decode_(train)\ntest = decode_(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop original nom_5 to nom_9\ntrain.drop(nom_col[5:], axis=1, inplace=True)\ntest.drop(nom_col[5:], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ordinal Variable\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ord_col = ['ord_0','ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\ntrain[ord_col].nunique()  #find out number of distinct categories for each feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ordinal_0"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['ord_0'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*ord_0* features are already labeled in an ordinal manner, so we can skip the encoding"},{"metadata":{},"cell_type":"markdown","source":" #### Ordinal_1 to Ordinal_4"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['ord_1'].value_counts())\nprint(train['ord_2'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the data, *ord_3* and *ord_4* are made up of ordered ASCII character. So we will encode them into respective ASCII code"},{"metadata":{"trusted":true},"cell_type":"code","source":"map1 = {'Novice':1, 'Contributor':2, 'Expert':3, 'Master':4, 'Grandmaster':5}\nmap2 = {'Freezing':1, 'Cold':2, 'Warm':3, 'Hot':4, 'Boiling Hot':5, 'Lava Hot':6}\n# map3 = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, \n#                 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15}\n# map4 = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, \n#                 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15,\n#                 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, \n#                 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n\ntrain['ord_1_encode'] = train['ord_1'].map(map1)\ntest['ord_1_encode'] = test['ord_1'].map(map1)\ntrain['ord_2_encode'] = train['ord_2'].map(map2)\ntest['ord_2_encode'] = test['ord_2'].map(map2)\ntrain['ord_3_encode'] = train['ord_3'].apply(lambda x: ord(x))\ntest['ord_3_encode'] = test['ord_3'].apply(lambda x: ord(x))\ntrain['ord_4_encode'] = train['ord_4'].apply(lambda x: ord(x))\ntest['ord_4_encode'] = test['ord_4'].apply(lambda x: ord(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ordinal_5 <br>\nThe intuition here is that we see the nature of ordinality in accordance to the assigned ASCII code number..<br>\nIn this case, we split the 2-char string into individual char, before mapping it into our pre-defined mapper by multiplying the ASCII numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split(word): \n    lst = [char for char in word]\n    return ','.join(lst)\n\ntrain['ord_5'] = train['ord_5'].apply(lambda x: split(x))\ntest['ord_5'] = test['ord_5'].apply(lambda x: split(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['ord_5_1','ord_5_2']] = train['ord_5'].str.split(',',expand=True)\ntest[['ord_5_1','ord_5_2']] = test['ord_5'].str.split(',',expand=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['ord_5_1_encode'] = train['ord_5_1'].apply(lambda x: ord(x))\ntest['ord_5_1_encode'] = test['ord_5_1'].apply(lambda x: ord(x))\n\ntrain['ord_5_2_encode'] = train['ord_5_2'].apply(lambda x: ord(x))\ntest['ord_5_2_encode'] = test['ord_5_2'].apply(lambda x: ord(x))\n\n\n#multiple the values together (1st char higher weightage)\n# train['ord_5_encode'] = (train['ord_5_1_encode'] * 100) + train['ord_5_2_encode']\n# test['ord_5_encode'] = (test['ord_5_1_encode'] * 100) + test['ord_5_2_encode']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cyclical Variables <br>\nhttps://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['day_encode_sin'] = np.sin(2 * np.pi * train['day']/7.0)\n# train['day_encode_cos'] = np.cos(2 * np.pi * train['day']/7.0)\n# train['month_encode_sin'] = np.sin(2 * np.pi * train['month']/12.0)\n# train['month_encode_cos'] = np.cos(2 * np.pi * train['month']/12.0)\n\n# test['day_encode_sin'] = np.sin(2 * np.pi * test['day']/7.0)\n# test['day_encode_cos'] = np.cos(2 * np.pi * test['day']/7.0)\n# test['month_encode_sin'] = np.sin(2 * np.pi * test['month']/12.0)\n# test['month_encode_cos'] = np.cos(2 * np.pi * test['month']/12.0)\n\ntrain = pd.get_dummies(train, prefix=['day','month'], columns=['day','month'],sparse=True)   #one-hot apparently works better than using cyclical encoding\ntest = pd.get_dummies(test, prefix=['day','month'], columns=['day','month'],sparse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop unencoded variables\ntestid = test['id']\ntrain.drop(['id','bin_0','ord_1','ord_2','ord_3','ord_4','ord_5','ord_5_1','ord_5_2'],axis=1,inplace=True)\ntest.drop(['id','bin_0','ord_1','ord_2','ord_3','ord_4','ord_5','ord_5_1','ord_5_2'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.columns)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscale_col = ['ord_3_encode','ord_4_encode','ord_5_1_encode','ord_5_2_encode']\nscaler = MinMaxScaler()\nscaler.fit(train[scale_col])\n\n\ntrain[scale_col] = scaler.transform(train[scale_col])\ntest[scale_col] = scaler.transform(test[scale_col] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get feature vector and target vector\ny = train['target']\nX = train.drop(['target'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split train to train validation set\ntrainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\ntrainX, trainy = oversample.fit_resample(trainX, trainy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reduce density of matrix by introducing sparsity\n# X = X.to_sparse()\n# X = X.sparse.to_coo()\n# X = X.astype(int)\n# X = X.tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(trainX.shape)\nprint(trainy.shape)\nprint(testX.shape)\nprint(testy.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kf=StratifiedKFold(n_splits=10)\n\n# def objective(trial):\n#     C=trial.suggest_loguniform('C', 10e-10, 1)\n#     model=linear_model.LogisticRegression(C=C, class_weight='balanced',max_iter=2000, solver='lbfgs', n_jobs=-1)\n#     score=-cross_val_score(model, trainX, trainy, cv=kf, scoring='roc_auc').mean()\n#     return score\n# study=optuna.create_study()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#study.optimize(objective, n_trials=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(study.best_params)\n# print(-study.best_value)\n# params=study.best_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression <br>\nApparently performs on-par or sometimes even better than other complex models\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores = []\n# glm = linear_model.LogisticRegression(C=0.1,solver=\"liblinear\",tol=0.00001, max_iter=10000)\n# cv = KFold(n_splits=10, random_state=42)\n# for tr_idx, val_idx in cv.split(trainX):\n#     train_x = trainX.iloc[tr_idx]\n#     train_y = trainy.iloc[tr_idx]\n#     val_x = trainX.iloc[val_idx]\n#     val_y = trainy.iloc[val_idx]\n\n#     glm.fit(train_x,train_y)\n#     y_pred_lr=glm.predict_proba(train_x)\n#     scores.append(roc_auc_score(train_y, y_pred_lr[:,1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nglm = linear_model.LogisticRegression(C=0.095,solver=\"liblinear\",tol=0.00001, max_iter=10000) \nglm.fit(trainX, trainy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_lr=glm.predict_proba(trainX)\nroc_auc_score(trainy, y_pred_lr[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_lr=glm.predict_proba(testX)\nroc_auc_score(testy, y_pred_lr[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\n\ny_preds = glm.predict_proba(testX)\ny_pred = np.where(y_preds > 0.5, 1, 0)\nconf = confusion_matrix(testy, y_pred)\nplt.imshow(conf, cmap='binary', interpolation='None')\nplt.show()\n\nprint(classification_report(testy, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\ndisp = plot_confusion_matrix(glm, testX, testy,\n                            cmap=plt.cm.Blues,\n                            normalize='true')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = glm.predict_proba(test)\nsubmission = pd.DataFrame({'id': testid, 'target': sub[:,1]})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Discriminant Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nlda = LDA()\nlda.fit(trainX, trainy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_lda=lda.predict_proba(trainX)\nroc_auc_score(trainy, y_pred_lda[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_lda=lda.predict_proba(testX)\nroc_auc_score(testy, y_pred_lda[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disp = plot_confusion_matrix(lda, testX, testy,\n                            cmap=plt.cm.Blues,\n                            normalize='true')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier as ada\nboost = ada()\nboost.fit(trainX,trainy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_ada=boost.predict_proba(testX)\nroc_auc_score(testy, y_pred_ada[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disp = plot_confusion_matrix(boost, testX, testy,\n                            cmap=plt.cm.Blues,\n                            normalize='true')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import ComplementNB \nnb = ComplementNB()\nnb.fit(trainX,trainy)\n\ny_pred_nb=nb.predict_proba(testX)\nroc_auc_score(testy, y_pred_nb[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disp = plot_confusion_matrix(nb, testX, testy,\n                            cmap=plt.cm.Blues,\n                            normalize='true')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import StackingClassifier\nestimator = [('lda', LDA()),\n             ('boost', ada()),\n             ('glm', LogisticRegression(C=0.095,solver=\"liblinear\",tol=0.00001, max_iter=10000))\n             ]\n\nclf = StackingClassifier(estimators=estimator, final_estimator=MLPClassifier(hidden_layer_sizes=(50,)), stack_method='predict_proba')\nclf.fit(trainX,trainy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_clf=clf.predict_proba(testX)\nroc_auc_score(testy, y_pred_clf[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}