{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"##### This Python 3 environment comes with many helpful analytics libraries installed\n##### It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n##### For example, here's several helpful packages to load in \nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nimport category_encoders as ce\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn import linear_model\nfrom category_encoders import *\n\n\n##### Input data files are available in the \"../input/\" directory.\n##### For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n##### Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"#read in the files\nsample_submission = pd.read_csv(\"../input/cat-in-the-dat/sample_submission.csv\")\ntest = pd.read_csv(\"../input/cat-in-the-dat/test.csv\")\ntrain = pd.read_csv(\"../input/cat-in-the-dat/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of training data: {}\" .format(train.shape))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the features of the dataset consists of mainly **binary**, **nominal**, **ordinal** and **cyclical**(day,month) features <br>\n<br>\n**Binary**: Consists of only two unique values. e.g 1/0, True/False, Yes/No <br>\n**Nominal**: Values that does not have any ordering <br>\n**Ordinal**: Values with specific ordering <br>\n**Cyclical**: Values that are recurrent in nature"},{"metadata":{},"cell_type":"markdown","source":"#### Distribution of Target\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(x='target',y='target', data=train, estimator=lambda x:len(x)/len(train)*100)\nax.set(ylabel =\"Frequency in %\", title=\"distribution of target\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binary Variables <br>\nWe will first investigate the distribution of 0s and 1s in the Binary variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_col = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\n\n\ngrid = gridspec.GridSpec(3, 2)\nplt.figure(figsize=(15,20))\nfor i,col in enumerate(train[bin_col]):\n    ax = plt.subplot(grid[i]) \n    sns.countplot(x=col, data=train) \n    ax.set_ylabel('Count')  \n    ax.set_xlabel('Values') \n    ax.set_title('{} Distribution'.format(col))\n    sizes=[] \n    for p in ax.patches:\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/len(train)*100),\n                ha=\"center\", fontsize=14) \n    ax.set_ylim(0, max(sizes) * 1.15) \n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Preprocessing Binary Variables <br>\nMachine Learning algorithm works on numerical data, hence, we will have to map boolean values from *bin_3* and *bin_4* into 0s and 1s"},{"metadata":{"trusted":true},"cell_type":"code","source":"mapper = {'T': 1, 'F':0, 'Y':1, 'N':0}\n#map char into 0/1\ntrain['bin_3'] = train['bin_3'].map(mapper)\ntest['bin_3'] = test['bin_3'].map(mapper)\ntrain['bin_4'] = train['bin_4'].map(mapper)\ntest['bin_4'] = test['bin_4'].map(mapper)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['bin_3'].value_counts())\nprint(train['bin_4'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{},"cell_type":"markdown","source":"### Nominal Variables <br>\nWe will try to visualize the values and its distribution for the nominal variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"nom_col = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n\nfor col in nom_col:\n    print(\"Number of unique values for {}: {}\".format(col,train[col].nunique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that *nom_5* to *nom_9* has high cardianality (alot of unique values). For visualisation purposes, we will just look at the nominal features with less than 10 unique values[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,20))\ngrid = gridspec.GridSpec(3, 2)\n\nfor i,col in enumerate(train[nom_col[:5]]):\n    ax = plt.subplot(grid[i]) \n    sns.countplot(x=col, data=train) \n    ax.set_ylabel('Count')  \n    ax.set_xlabel('Values') \n    ax.set_title('{} Distribution'.format(col))\n    sizes=[] \n    for p in ax.patches:\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/len(train)*100),\n                ha=\"center\", fontsize=14) \n    ax.set_ylim(0, max(sizes) * 1.15) \n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### One-Hot encoding <br>\nWe will use one-hot encoding in this case for *nom_0* to *nom_4*. One-hot encoding will create K number os columns based on the unique values and assign a binary value indicating the presence of the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot(df):\n    df_work = df[['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']]\n    df_result = pd.get_dummies(df_work, prefix=nom_col[0:5], columns=nom_col[0:5])\n    df = pd.concat([df,df_result],axis=1)\n    return df\n\ntrain = one_hot(train)\ntest = one_hot(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Nominal Features with high cardianality <br>\nLets look at *nom_5* to *nom_9* values to find out more about the high cardianality features. They are mainly made up of alphanumeic strings that doesnt have any meaning\n"},{"metadata":{},"cell_type":"markdown","source":"In this case, i will try to use:\n1. **Hashing**\n2. **Target Encoding**\n3. **Decoding hexademical**"},{"metadata":{},"cell_type":"markdown","source":"#### Hashing (experiment)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for col in nom_col[5:]:\n#     col_name = col+'_hash_encode'\n#     train[col_name] = train[col].apply(lambda x:hash(str(x)) % 6011)\n#     test[col_name] = test[col].apply(lambda x:hash(str(x)) % 6011)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Target Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import TargetEncoder\nencoder = TargetEncoder()\nfor col in nom_col[5:]:\n    colname = col+'_target'\n    train[colname] = encoder.fit_transform(train['nom_5'], train['target'])\n    test[colname] = encoder.transform(test['nom_5'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decoding Hexadecimal values (experiment)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #function to decode hexa to binary\n# def hex_to_bin(val):\n#     base = 16\n#     num_bits = 36\n#     res = bin(int(val, base))[2:].zfill(num_bits)\n#     return str(res)\n\n# #split bin values into individual columns\n# def decode_(df):    \n#     for col in nom_col[5:]:\n#         df['new_col'] = df[col].apply(lambda x:hex_to_bin(x))\n#         df['new_col'] = df['new_col'].apply(lambda x:list(x))\n#         new_col = pd.DataFrame(df['new_col'].values.tolist()).rename(columns = lambda x: col+'_decode_'+str(x))\n#         df = pd.concat([df,new_col],axis=1)\n        \n#     return df\n\n# #run for test and train\n# train = decode_(train)\n# test = decode_(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ordinal Variable\n"},{"metadata":{},"cell_type":"markdown","source":"Visualize the first 3 Ordinal columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"ord_col = ['ord_0','ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\ntrain[ord_col].nunique()  #find out number of distinct categories for each feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,20))\ngrid = gridspec.GridSpec(3, 2)\n\nfor i,col in enumerate(train[ord_col[:3]]):\n    ax = plt.subplot(grid[i]) \n    sns.countplot(x=col, data=train) \n    ax.set_ylabel('Count')  \n    ax.set_xlabel('Values') \n    ax.set_title('{} Distribution'.format(col))\n    sizes=[] \n    for p in ax.patches:\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/len(train)*100),\n                ha=\"center\", fontsize=14) \n    ax.set_ylim(0, max(sizes) * 1.15) \n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ordinal_0"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['ord_0'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*ord_0* features are already labeled in an ordinal manner, so we can skip the encoding"},{"metadata":{},"cell_type":"markdown","source":" #### Ordinal_1 to Ordinal_4"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['ord_1'].value_counts())\nprint(train['ord_2'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the data, *ord_3* and *ord_4* are made up of ordered ASCII character. So we will encode them into respective ASCII code"},{"metadata":{"trusted":true},"cell_type":"code","source":"map1 = {'Novice':1, 'Contributor':2, 'Expert':3, 'Master':4, 'Grandmaster':5}\nmap2 = {'Freezing':1, 'Cold':2, 'Warm':3, 'Hot':4, 'Boiling Hot':5, 'Lava Hot':6}\n\n\ntrain['ord_1_encode'] = train['ord_1'].map(map1)\ntest['ord_1_encode'] = test['ord_1'].map(map1)\ntrain['ord_2_encode'] = train['ord_2'].map(map2)\ntest['ord_2_encode'] = test['ord_2'].map(map2)\ntrain['ord_3_encode'] = train['ord_3'].apply(lambda x: ord(x))\ntest['ord_3_encode'] = test['ord_3'].apply(lambda x: ord(x))\ntrain['ord_4_encode'] = train['ord_4'].apply(lambda x: ord(x))\ntest['ord_4_encode'] = test['ord_4'].apply(lambda x: ord(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ordinal_5 <br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def freq_encoding(df):\n    col_name = 'ord_5'+'_freq_encode'\n    fe_train = df.groupby(['ord_5']).size()/len(df)\n    df[col_name] = df['ord_5'].map(fe_train)\n\nfreq_encoding(train)\nfreq_encoding(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The intuition here is that we see the nature of ordinality in accordance to the assigned ASCII code number..<br>\nIn this case, we split the 2-char string into individual char, before mapping it into our pre-defined mapper by multiplying the ASCII numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split(word): \n    lst = [char for char in word]\n    return ','.join(lst)\n\ntrain['ord_5'] = train['ord_5'].apply(lambda x: split(x))\ntest['ord_5'] = test['ord_5'].apply(lambda x: split(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['ord_5_1','ord_5_2']] = train['ord_5'].str.split(',',expand=True)\ntest[['ord_5_1','ord_5_2']] = test['ord_5'].str.split(',',expand=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['ord_5_1_encode'] = train['ord_5_1'].apply(lambda x: ord(x))\ntest['ord_5_1_encode'] = test['ord_5_1'].apply(lambda x: ord(x))\n\ntrain['ord_5_2_encode'] = train['ord_5_2'].apply(lambda x: ord(x))\ntest['ord_5_2_encode'] = test['ord_5_2'].apply(lambda x: ord(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One-hot encoding (experiment)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = pd.get_dummies(train, prefix=['ord_5_1','ord_5_2'], columns=['ord_5_1','ord_5_2'],sparse=True)   #one-hot apparently works better than using cyclical encoding\n# test = pd.get_dummies(test, prefix=['ord_5_1','ord_5_2'], columns=['ord_5_1','ord_5_2'],sparse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One-Hot Encoding for Time <br>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.get_dummies(train, prefix=['day','month'], columns=['day','month'],sparse=True)   #one-hot apparently works better than using cyclical encoding\ntest = pd.get_dummies(test, prefix=['day','month'], columns=['day','month'],sparse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cyclic Encoding (experiment)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['day_encode_sin'] = np.sin(2 * np.pi * train['day']/7.0)\n# train['day_encode_cos'] = np.cos(2 * np.pi * train['day']/7.0)\n# train['month_encode_sin'] = np.sin(2 * np.pi * train['month']/12.0)\n# train['month_encode_cos'] = np.cos(2 * np.pi * train['month']/12.0)\n\n# test['day_encode_sin'] = np.sin(2 * np.pi * test['day']/7.0)\n# test['day_encode_cos'] = np.cos(2 * np.pi * test['day']/7.0)\n# test['month_encode_sin'] = np.sin(2 * np.pi * test['month']/12.0)\n# test['month_encode_cos'] = np.cos(2 * np.pi * test['month']/12.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Drop useless Columns\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop unencoded variables\ntestid = test['id']\ntrain.drop(['id','bin_0','ord_1','ord_2','ord_3','ord_4','nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4','ord_5_1','ord_5_2'],axis=1,inplace=True)\ntest.drop(['id','bin_0','ord_1','ord_2','ord_3','ord_4','nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4','ord_5_1','ord_5_2'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.columns)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get feature vector and target vector\ny = train['target']\nX = train.drop(['target'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Weight of Evidence Encoding\nenc = WOEEncoder(cols=['ord_5', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']).fit(X, y)\nX = enc.transform(X)\ntest = enc.transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscale_col = ['ord_3_encode','ord_4_encode', 'ord_5_freq_encode','ord_5', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\nscaler = MinMaxScaler()\nscaler.fit(X[scale_col])\n\n\nX[scale_col] = scaler.transform(X[scale_col])\ntest[scale_col] = scaler.transform(test[scale_col] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split train to train validation set\ntrainX_init, testX, trainy_init, testy = train_test_split(X, y, test_size=0.2, random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SMOTE Oversampling our minority class (1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\ntrainX, trainy = SMOTE(random_state = 2020).fit_resample(trainX_init, trainy_init)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"before_smote = pd.DataFrame(trainy_init, columns=['target'])\nafter_smote = pd.DataFrame(trainy, columns=['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot the distribution of Target 'Before' and 'After' SMOTE "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(figsize=(20,8),ncols=2, nrows=1)\nax1.set_title('Before SMOTE')\nax2.set_title('After SMOTE')\nsns.barplot(x='target',y='target', data=before_smote, estimator=lambda x:len(x)/len(before_smote)*100,ax=ax1)\nsns.barplot(x='target',y='target', data=after_smote, estimator=lambda x:len(x)/len(after_smote)*100,ax=ax2)\nax1.set_ylabel('Percentage %')\nax2.set_ylabel('Percentage %')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainy.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(trainX.shape)\nprint(trainy.shape)\nprint(testX.shape)\nprint(testy.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression <br>\nApparently performs on-par or sometimes even better than other complex models\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"glm = linear_model.LogisticRegression(C=0.1,solver=\"liblinear\", penalty='l2',random_state=2020) \nglm.fit(trainX, trainy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_lr=glm.predict_proba(trainX)\nroc_auc_score(trainy, y_pred_lr[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_lr=glm.predict_proba(testX)\nroc_auc_score(testy, y_pred_lr[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\n\ndisp = plot_confusion_matrix(glm, testX, testy,\n                            cmap=plt.cm.Blues,\n                            normalize='true')\n\ny_preds = glm.predict(testX)\n\nprint(classification_report(testy, y_preds))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = glm.predict_proba(test)\nsubmission = pd.DataFrame({'id': testid, 'target': sub[:,1]})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}